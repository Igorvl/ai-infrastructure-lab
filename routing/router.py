import os
import json
import logging
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
from litellm import completion

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("AI-Gateway")

app = FastAPI(title="AI Infrastructure Gateway v3.0")

# --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ---
CONFIG_PATH = os.getenv("CONFIG_PATH", "deploy/antigravity.json")
try:
    with open(CONFIG_PATH, "r") as f:
        CONFIG = json.load(f)
    logger.info(f"‚úÖ Configuration loaded from {CONFIG_PATH}")
except Exception as e:
    logger.error(f"‚ùå Failed to load config: {e}")
    CONFIG = {"models": {}}

# --- –ú–û–î–ï–õ–ò –î–ê–ù–ù–´–• ---
class Message(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    model: str
    messages: List[Message]
    temperature: Optional[float] = 0.7
    max_tokens: Optional[int] = None

# --- –≠–ù–î–ü–û–ò–ù–¢–´ ---
@app.get("/health")
async def health_check():
    return {"status": "operational", "models_loaded": list(CONFIG.get("models", {}).keys())}

@app.get("/v1/models")
async def list_models():
    data = []
    for model_id, params in CONFIG.get("models", {}).items():
        data.append({
            "id": model_id,
            "object": "model",
            "created": 1677610602,
            "owned_by": params.get("provider", "unknown"),
            "name": f"{model_id} ({params.get('model_name')})"
        })
    return {"object": "list", "data": data}

@app.post("/v1/chat/completions")
async def chat_completions(request: ChatRequest):
    requested_model = request.model
    
    # 1. Fallback –ª–æ–≥–∏–∫–∞ –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–∏
    if requested_model not in CONFIG["models"]:
        logger.warning(f"Requested model '{requested_model}' not found. Defaulting to Primary.")
        target_role = CONFIG["fallback_order"][0]
    else:
        target_role = requested_model

    model_cfg = CONFIG["models"][target_role]
    
    # --- –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–Ø –ò–ú–ï–ù–ò –ú–û–î–ï–õ–ò ---
    provider = model_cfg["provider"]
    real_model_name = model_cfg["model_name"]
    
    # LiteLLM —Ç—Ä–µ–±—É–µ—Ç —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø—Ä–µ—Ñ–∏–∫—Å—ã
    if provider == "google":
        # –ú–µ–Ω—è–µ–º 'google' –Ω–∞ 'gemini'
        litellm_model = f"gemini/{real_model_name}"
    elif provider == "openai":
        # –î–ª—è OpenAI-compatible (DeepSeek/Qwen) –ø—Ä–µ—Ñ–∏–∫—Å —á–∞—Å—Ç–æ –Ω–µ –Ω—É–∂–µ–Ω –∏–ª–∏ openai/
        litellm_model = real_model_name
    elif provider == "zhipu":
        # Zhipu AI
        litellm_model = f"zhipu/{real_model_name}"
    else:
        # Default: provider/model
        litellm_model = f"{provider}/{real_model_name}"

    # 3. –°–æ–±–∏—Ä–∞–µ–º –∞—Ä–≥—É–º–µ–Ω—Ç—ã
    kwargs = {
        "model": litellm_model,
        "messages": [m.dict() for m in request.messages],
        "temperature": request.temperature,
        "max_tokens": request.max_tokens or model_cfg.get("max_tokens", 4096),
        "api_key": os.getenv(model_cfg.get("api_key_env")),
    }

    if "api_base" in model_cfg:
        kwargs["api_base"] = model_cfg["api_base"]
    
    if "extra_body" in model_cfg:
        kwargs["extra_body"] = model_cfg["extra_body"]

    logger.info(f"üöÄ Routing: {target_role} -> {litellm_model}")

    try:
        # –í—ã–∑–æ–≤ LiteLLM
        response = completion(**kwargs)
        return response
        
    except Exception as e:
        logger.error(f"üî• Error calling {target_role}: {str(e)}")
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º 500, —á—Ç–æ–±—ã –±—ã–ª–æ –≤–∏–¥–Ω–æ –≤ –ª–æ–≥–∞—Ö –∫–ª–∏–µ–Ω—Ç–∞, –Ω–æ –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å Fallback
        raise HTTPException(status_code=500, detail=f"Provider Error: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
